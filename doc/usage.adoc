ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

:toc:
:toc-placement!:

= Using `collectionspace-mapper`

toc::[]

== Setup

Requires:

- collectionspace-client
- collectionspace-refcache

Examples of setting up client and refcache are available in https://github.com/collectionspace/collectionspace-refcache/blob/master/doc/REFCACHE.md[the REFCACHE doc] and in https://github.com/collectionspace/collectionspace-mapper/blob/master/spec/helpers.rb[`/spec/helpers.rb`] in this repo.

- a `cspace-config-untangler` RecordMapper, parsed

RecordMapper JSON files for all current profile/rectype combinations can be found in: https://github.com/collectionspace/collectionspace-mapper/tree/master/spec/fixtures/files/mappers[`spec/fixtures/files/mappers`].

These JSON files should be parsed before passing to this application.

Sample simple setup could look like: 

[source,ruby]
----
client = CollectionSpace::Client.new(
  CollectionSpace::Configuration.new(
    base_uri: 'https://core.dev.collectionspace.org/cspace-services',
    username: 'admin@core.collectionspace.org',
    password: 'Administrator'
  )
)

cache_config = { domain: 'core.collectionspace.org' }
cache = CollectionSpace::RefCache.new(config: cache_config, client: client)

record_mapper = JSON.parse(File.read('spec/fixtures/files/mappers/release_6_1/core/core_6_1_0-collectionobject.json'))
----

== Create DataHandler for batch

The DataHandler object sets up all the stuff that only needs to be done once per batch, regardless of how many records are in the batch.

You also send each row/record through DataHandler methods for processing.

=== DataHandler config parameter

A dataset-specific configuration hash may be passed in when creating a new DataHandler. This hash contains settings that control how the Mapper will parsed and transform the data.

Details on available config options and how to format them are in the **Config options** section below.

If no config parameter is passed in, the following minimum required default configuration will be applied:

.Default config
[source,ruby]
----
{ delimiter: ';',
  subgroup_delimiter: '^^',
  response_mode: 'normal',
  force_defaults: false,
  date_format: 'month day year'
}
----

[NOTE]
====
Default delimiter `;` will be replaced with `|` after DDD are finished contributing to test coverage.
====

[source,ruby]
.Creating a DataHandler with default config
----
handler = DataHandler.new(record_mapper, client, cache)
----

[source,ruby]
.Creating a DataHandler with custom batch config
----
config = {
  delimiter: '%',
  subgroup_delimiter: '~'
}

handler = DataHandler.new(record_mapper, client, cache, config)
----


== Row/record data

=== Format

Each row or record of data to be mapped should be passed in as a Hash.

Hash keys are Strings and should match headers from the CSV templates output by `cspace-config-untangler`. Keys that do not match CSV headers will not be mapped. 

Hash values are Strings. You can also pass through null Hash values.

Example data Hashes used for testing can be found https://github.com/collectionspace/collectionspace-mapper/tree/master/spec/fixtures/files/datahashes[here].

=== Checking CSV headers/fields

Send one record through the following before beginning other validation. The results of this check belong more with the row/column count of the CSV file than any information about individual records.

[source,ruby]
----
handler.check_fields(data)
 => {
        :known_fields=>['conservationnumber', 'status'],
        :unknown_fields=>['conservator']
     }
----

A warning about the unknown `conservator` field will be very valuable to a user who doesn't realize they must use `conservatorperson` or `conservatororganization` as their CSV headers. 

=== Validating a record

Currently validation only checks for the presence of required field(s) and values in required field(s).

[source,ruby]
.Valid record
----
data = { 'objectNumber' => '123', 'recordStatus' => 'test' }
validated = handler.validate(data) # <1>
errors = validated.errors # <2>
valid = validated.valid? # <3>
----
<1> Returns a CollectionSpace::Mapper::Response object
<2> Returns empty Array
<3> Returns Boolean true

[source,ruby]
.Invalid record - required field missing
----
data = { 'objectId' => '123', 'recordStatus' => 'test' }
validated = handler.validate(data)
errors = validated.errors # <1>
valid = validated.valid? # <2>
----
<1> Returns Array containing one error message String (shown below)
<2> Returns Boolean false

[source,ruby]
.errors - Array of error message Strings - example for missing field
----
['required field missing: objectNumber must be present']
----

[source,ruby]
.Invalid record - required field present but empty
----
data = { 'objectNumber' => '', 'recordStatus' => 'test' }
validated = handler.validate(data)
errors = validated.errors # <1>
valid = validated.valid? # <2>
----
<1> Returns Array containing one error message String (shown below)
<2> Returns Boolean false

[source,ruby]
.errors - Array of error message Strings - example for empty field
----
['required field empty: objectNumber must be populated']
----

=== Processing a record

[CAUTION]
====
Processing an invalid record will have unpredictable results and may completely blow up.

Failure is not graceful yet.
====

Processing a record causes the following to happen:

- All values become Arrays, with the values of multivalued fields as separate array elements. Single value fields become Arrays with length == 1.
- Values are transformed according to instructions hardcoded in the RecordMapper, and any optional batch-specific transformed sent in as part of the `Mapper::DataHandler`'s `config` parameter.
- Data quality checks are performed. Warnings may be included in the response.
- Data Hash values which map to the same CollectionSpace XML field (i.e. separate CSV columns for field values from different authorities) are combined into one field.
- CollectionSpace XML document is produced

[source,ruby]
.Validating and processing a record (Option 1)
----
data = { 'objectNumber' => '123', 'recordStatus' => 'test' }
validated = handler.validate(data)
valid = validated.valid?
 => true
processed = handler.process(data) # <1>
----
<1> We pass in the same data hash we validated. This returns a CollectionSpace::Mapper::Response object

[NOTE]
====
Given `DataHandler.validate`'s current behavior, the above will work. However, if any functionality is ever built into that method such that it might be desirable to keep data from the `validate` response with the processed data, then it's probably safer to use Option 2:
====

[source,ruby]
.Validating and processing a record (Option 2)
----
data = { 'objectNumber' => '123', 'recordStatus' => 'test' }
validated = handler.validate(data)
valid = validated.valid?
 => true
processed = handler.process(validated) # <1>
----
<1> We pass in the Mapper::Reponse returned by validation. This returns a CollectionSpace::Mapper::Response object

=== Working with `Mapper::Response`

`handler.process(data)` will return a `Mapper::Response` that looks something like the following.

If your `config[:response_mode]` is set to `verbose`, you will also receive the full data hashes from each stage of processing (`@orig_data`, `@combined_data`, etc.).

`processed.doc`:: Returns the mapped CollectionSpace XML as a `Nokogiri::XML::Document`. If `nil`, check for errors.
`processed.identifier`:: Returns the value of the field specified as the record identifier
`processed.errors`:: Returns Array of errors. Since you should not process invalid data, any errors in a response from processing will indicate problems in the mapping.
`processed.warnings`:: Returns Array of data quality warnings
`processed.terms`:: Returns Array of Hashes, each containing information about one authority or vocabulary term in the record.

[source,ruby]
.Anatomy of a `Mapper::Response` when `config[:response_mode]` = `normal`
----
pp(processed)
  =>
  #<CollectionSpace::Mapper::Response:0x00007ff8ab308120
     @combined_data={},
     @doc=
      #(Document:0x3ffc5598c268 {
        name = "document",
        children = [
          #(Element:0x3ffc5598c18c {
            name = "document",
            children = [
              #(Element:0x3ffc5598c0ec {
                name = "ns2:collectionobjects_common",
                children = [
                  #(Element:0x3ffc5598c09c {
                    name = "objectNumber",
                    children = [ #(Text "123")]
                    }),
                  #(Element:0x3ffc5598c024 {
                    name = "recordStatus",
                    children = [ #(Text "test")]
                    })]
                })]
            })]
        }),
     @errors=[],
     @identifier="123",
     @merged_data={},
     @orig_data={},
     @split_data={},
     @transformed_data={},
     @terms=
      [{:category=>:vocabulary,
         :field=>"titletranslationlanguage",
         :type=>"vocabularies",
         :subtype=>"languages",
         :value=>"Ancient Greek",
         :found=>true},
        {:category=>:vocabulary,
         :field=>"titletranslationlanguage",
         :type=>"vocabularies",
         :subtype=>"languages",
         :value=>"Swahili",
         :found=>true},
        {:category=>:vocabulary,
         :field=>"titletranslationlanguage",
         :type=>"vocabularies",
         :subtype=>"languages",
         :value=>"Klingon",
         :found=>false},
        {:category=>:vocabulary,
         :field=>"titletranslationlanguage",
         :type=>"vocabularies",
         :subtype=>"languages",
         :value=>"Spanish",
         :found=>true}],
     @warnings=
      [{:category=>:unknown_option_list_value,
        :field=>"recordstatus",
        :type=>"option list value",
        :subtype=>"",
        :value=>"test",
        :message=>"Unknown value in option list `recordstatus` column"}]>
----

== Config options

A JSON config hash may be passed to a new `Mapper::DataHandler` to control various aspects of the data transformation. 

.Example config JSON
[source,javascript]
----
{
  "delimiter": ";",
  "subgroup_delimiter": "^^",
  "response_mode": "verbose",
  "force_defaults": false,
  "date_format": "month day year",
  "two_digit_year_handling": "convert to four digit",
  "transforms": {
    "collection": {
      "special": [
        "downcase_value"
      ],
      "replacements": [{
        "find": " ",
        "replace": "-",
        "type": "plain"
      }]
    }
  },
  "default_values": {
    "publishTo": "DPLA;Omeka",
    "collection": "library-collection"
  }
}
----

=== delimiter

Delimiter character or string used to split repeatable values within the cell of a CSV. 

- *Required?:* yes
- *Defaults to:* |
- *Data type*: string

=== subgroup_delimiter

Delimiter character or string used to split repeatable values nested inside other repeatable values (example: titleTranslation, titleTranslationLanguage).

This is only used when if you are importing data into a repeatable field group within a larger repeatable field group.

- *Required?:* yes
- *Defaults to:* ^^
- *Data type*: string

=== response_mode

If `normal`, `Mapper::Response.orig_data` returns the original data hash, and `Mapper::Response.doc` returns the resulting XML document.

If `verbose`, `Mapper::Response` also has the following attributes, which may be helpful in debugging:

- `.merged_data` - result of merging any default values into `orig_data`.
- `.split_data` - result of splitting `merged_data` using `delimiter` and `subgroup_delimiter`. All field values are now arrays.
- `.transformed_data` - result of any transformations applied to `split_data`.
- `.combined_data` - result of combining separate data columns (such as `approvedByPerson` and `approvedByOrganization`) into one CollectionSpace field (`approvedBy`).  

- *Required?:* yes
- *Defaults to:* normal
- *Data type*: string
- *Allowed values*: `normal`, `verbose`

=== force_defaults

Only has an effect if you are also providing `default_values` in your config.

Relevant if some fields for which you are providing `default_values` have other values in the source data (CSV). 

If `false`, default values will not replace or be added to values passed in via the data hash; default value will be inserted if field is missing or empty in data hash.

If `true`, default value will replace any data hash values.

- *Required?:* yes
- *Defaults to:* false
- *Data type*: boolean
- *Allowed values*: `true`, `false`

=== date_format

Only has an effect on dates like: 3/4/2020 or 03-04-2020.

If `month day year`, these dates would be interpreted as March 4, 2020.

If `day month year`, these dates would be interpreted as April 3, 2020.

- *Required?:* yes
- *Defaults to:* `month day year`
- *Data type*: string
- *Allowed values*: `month day year`, `day month year`

=== two_digit_year_handling

Only has an effect on dates like: 1-21-19 or 1-21-45, where a four digit year is not provided.

Entering such dates in CollectionSpace manually would result in the years being parsed as 0019 and 0045.

Setting this to `literal` will keep that behavior.

Setting this to `coerce` results in the years being parsed as 2019 and 1945 via the following algorithm:

- get the current year
- if the two-digit year in the data is less than or equal to the last two digits of the current year, use the first two digits of the current year as the first two digits of the coerced four-digit year.
- if the two-digit year in the data is greater than the last two digits of the current year, use the first two digits of the current year *minus one* as the first two digits of the coerced four-digit year.

- *Required?:* yes
- *Defaults to:* `coerce`
- *Data type*: string
- *Allowed values*: `coerce`, `literal`



- `:transforms` - optional - Hash - Key (String) is the data hash field to which the transforms should be applied. Value (Hash) is structured transformation instructions.
- `:default_values` - optional - Hash - Key (String) is the data hash field that should be populated. Value (String) is the default value for the field.


